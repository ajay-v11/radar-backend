# Visibility Orchestrator

A LangGraph-based orchestrator that chains together 3 agents for complete visibility analysis.

## Quick Start

### 1. Test with Cached Data (Recommended)

```bash
python test_visibility_orchestration.py
```

This uses the cached company data from `test_dynamic_result.json` (fast, ~30 seconds).

### 2. Test with Fresh Data

```bash
python test_visibility_orchestration.py --fresh
```

This runs industry detection first, then orchestration (slow, ~60 seconds).

## What It Does

```
Query Generator → AI Model Tester → Scorer Analyzer
```

1. **Query Generator**: Uses dynamic `query_categories_template` from industry detector
2. **AI Model Tester**: Tests queries across multiple AI models
3. **Scorer Analyzer**: Calculates visibility score with detailed analytics

## Key Feature: Dynamic Query Categories ⭐

The orchestrator uses the `query_categories_template` generated by the industry detector:

```python
# Industry detector generates this dynamically
query_categories_template = {
    "product_comparison": {
        "name": "Product Comparison",
        "weight": 0.30,
        "description": "Users comparing products",
        "examples": ["Amazon vs eBay"]
    },
    # ... more categories specific to this company
}

# Query generator uses it automatically
result = run_visibility_orchestration(
    company_data=company_data,  # Contains query_categories_template
    num_queries=20
)
```

## Usage in Code

```python
from agents.visibility_orchestrator import run_visibility_orchestration

# Get company data from Phase 1 (industry detection)
company_data = {
    "company_url": "https://example.com",
    "company_name": "Example Corp",
    "industry": "E-commerce",
    "competitors": ["Competitor A", "Competitor B"],
    "query_categories_template": {...}  # Dynamic template
}

# Run orchestration
result = run_visibility_orchestration(
    company_data=company_data,
    num_queries=20,
    models=["chatgpt", "gemini"],
    llm_provider="openai"
)

print(f"Visibility Score: {result['visibility_score']}%")
```

## Output

```python
{
    "queries": [...],              # Generated queries
    "query_categories": {...},     # Organized by category
    "model_responses": {...},      # Responses from each model
    "visibility_score": 85.5,      # Overall score
    "analysis_report": {           # Detailed analytics
        "by_model": {...},
        "by_category": {...},
        "competitor_rankings": {...},
        "query_log": [...]
    },
    "errors": []
}
```

## Architecture

- **Location**: `agents/visibility_orchestrator/`
- **Entry Point**: `run_visibility_orchestration()`
- **Graph**: Linear workflow (no branching)
- **State**: `VisibilityOrchestrationState` (unified state for all agents)

## Files

- `__init__.py` - Entry point
- `graph.py` - LangGraph workflow definition
- `models.py` - State TypedDict
- `nodes.py` - Node functions (wraps sub-agents)
- `README.md` - This file

## Testing

The test script (`test_visibility_orchestration.py`) shows:

- ✅ Query generation using dynamic categories
- ✅ Model testing across multiple models
- ✅ Score calculation with detailed analytics
- ✅ Category breakdown
- ✅ Competitor rankings
- ✅ Error handling

## Next Steps

After testing, integrate with API:

1. Update `src/controllers/analysis_controller.py`
2. Replace old agent calls with orchestrator
3. Update routes to use new orchestrator
4. Test end-to-end via API

See `docs/VISIBILITY_ORCHESTRATION.md` for full documentation.
